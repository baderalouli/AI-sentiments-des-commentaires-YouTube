{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'googleapiclient'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogleapiclient\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdiscovery\u001b[39;00m \u001b[39mimport\u001b[39;00m build\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogleapiclient\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39merrors\u001b[39;00m \u001b[39mimport\u001b[39;00m HttpError\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'googleapiclient'"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import pandas as pd\n",
    "\n",
    "# Mettez votre clé API YouTube ici\n",
    "api_key = \"AIzaSyC0ZXV9Oe2Byhikt-TD5AOPKC-3kFcfljE\"\n",
    "\n",
    "# Créez un objet service pour l'API YouTube\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Identifiant de la vidéo\n",
    "video_id = \"ycPr5-27vSI\"\n",
    "\n",
    "# Récupérez les commentaires de la vidéo\n",
    "comments = []\n",
    "results = youtube.commentThreads().list(part='snippet', videoId=video_id, textFormat='plainText').execute()\n",
    "\n",
    "while results:\n",
    "    for item in results['items']:\n",
    "        comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "        comments.append(comment)\n",
    "    if 'nextPageToken' in results:\n",
    "        results = youtube.commentThreads().list(part='snippet', videoId=video_id, pageToken=results['nextPageToken'], textFormat='plainText').execute()\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Créez un tableau à partir des commentaires récupérés\n",
    "data = pd.DataFrame(data=comments, columns=[\"Commentaires\"])\n",
    "\n",
    "# Enregistrez le tableau dans un fichier CSV\n",
    "data.to_csv(\"commentaires.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_comment(comment):\n",
    "    # Vérifier si la valeur est manquante\n",
    "    if pd.isnull(comment):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convertir en minuscules\n",
    "    comment = comment.lower()\n",
    "    \n",
    "    # Supprimer les caractères spéciaux et les chiffres\n",
    "    comment = ''.join(e for e in comment if e.isalnum() or e.isspace())\n",
    "    \n",
    "    # Tokenizer les mots\n",
    "    words = word_tokenize(comment)\n",
    "    \n",
    "    # Supprimer les stop words\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    # Stemming des mots\n",
    "    words = [ps.stem(w) for w in words]\n",
    "    \n",
    "    # Lemmatization des mots\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    \n",
    "    # Rejoindre les mots en une phrase\n",
    "    cleaned_comment = ' '.join(words)\n",
    "    \n",
    "    return cleaned_comment\n",
    "\n",
    "# Charger le fichier CSV dans un DataFrame\n",
    "df = pd.read_csv(\"commentaires.csv\")\n",
    "\n",
    "# Supprimer les lignes avec des valeurs non alphabétiques\n",
    "df = df[pd.notnull(df['Commentaires'])]\n",
    "df = df[df['Commentaires'].apply(lambda x: x.isalpha())]\n",
    "\n",
    "# Appliquer la fonction de nettoyage aux commentaires\n",
    "df['Commentaires nets'] = df['Commentaires'].apply(clean_comment)\n",
    "\n",
    "# Enregistrer le tableau nettoyé dans un fichier CSV\n",
    "df.to_csv(\"commentaires_nets.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Commentaires Commentaires nets sentiment\n",
      "0            Great             great  Positive\n",
      "1                W                 w   Neutral\n",
      "2         peculiar          peculiar   Neutral\n",
      "3             Haha              haha  Positive\n",
      "4     Thanksgiving         thanksgiv   Neutral\n",
      "...            ...               ...       ...\n",
      "1004          nice              nice  Positive\n",
      "1005           чмо               чмо   Neutral\n",
      "1006         first             first  Positive\n",
      "1007        Second            second   Neutral\n",
      "1008        second            second   Neutral\n",
      "\n",
      "[1009 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Lire le fichier CSV avec les commentaires\n",
    "comments_df = pd.read_csv('commentaires_nets.csv')\n",
    "\n",
    "# Analyser le sentiment de chaque commentaire\n",
    "for index, row in comments_df.iterrows():\n",
    "    # Créer un objet TextBlob pour le commentaire\n",
    "    blob = TextBlob(row['Commentaires'])\n",
    "    # Calculer la polarité du sentiment\n",
    "    sentiment_polarity = blob.sentiment.polarity\n",
    "    # Classer le commentaire en fonction de sa polarité\n",
    "    if sentiment_polarity > 0:\n",
    "        sentiment = 'Positive'\n",
    "    elif sentiment_polarity < 0:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    # Ajouter la colonne \"sentiment\" au DataFrame\n",
    "    comments_df.at[index, 'sentiment'] = sentiment\n",
    "\n",
    "# Afficher les commentaires avec leur sentiment\n",
    "print(comments_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from textblob import TextBlob\n",
    "import csv\n",
    "\n",
    "\n",
    "# Connect to Neo4j database\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "session = driver.session()\n",
    "\n",
    "# Open the CSV file containing the comments\n",
    "with open('comments.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    comments = list(reader)\n",
    "\n",
    "# Analyze the sentiment of each comment and store it in Neo4j\n",
    "for comment in comments:\n",
    "    # Create a TextBlob object for the comment\n",
    "    blob = TextBlob(comment[0])\n",
    "    # Calculate the polarity of the sentiment\n",
    "    polarity = blob.sentiment.polarity\n",
    "    # Classify the comment based on its polarity\n",
    "    if polarity > 0:\n",
    "        sentiment = \"Positive\"\n",
    "    elif polarity < 0:\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        sentiment = \"Neutral\"\n",
    "    # Create a node for the comment and store its sentiment score\n",
    "    result = session.run(\"CREATE (c:Comment {text: $text, sentiment: $sentiment})\", text=comment[0], sentiment=sentiment)\n",
    "\n",
    "# Close the Neo4j session\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
