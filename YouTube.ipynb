{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import pandas as pd\n",
    "\n",
    "# Mettez votre clé API YouTube ici\n",
    "api_key = \"AIzaSyAXflmi_XQTjHj1Kbe3vS9trK5086DlVQI\"\n",
    "\n",
    "# Créez un objet service pour l'API YouTube\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Identifiant de la vidéo\n",
    "video_id = \"YqvCptqhHfs\"\n",
    "\n",
    "# Récupérez les commentaires de la vidéo\n",
    "comments = []\n",
    "results = youtube.commentThreads().list(part='snippet', videoId=video_id, textFormat='plainText').execute()\n",
    "\n",
    "while results:\n",
    "    for item in results['items']:\n",
    "        comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "        comments.append(comment)\n",
    "    if 'nextPageToken' in results:\n",
    "        results = youtube.commentThreads().list(part='snippet', videoId=video_id, pageToken=results['nextPageToken'], textFormat='plainText').execute()\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Créez un tableau à partir des commentaires récupérés\n",
    "data = pd.DataFrame(data=comments, columns=[\"Commentaires\"])\n",
    "\n",
    "# Enregistrez le tableau dans un fichier CSV\n",
    "data.to_csv(\"commentaires.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u043e' in position 0: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m reader:\n\u001b[0;32m     49\u001b[0m     cleaned_row \u001b[39m=\u001b[39m [clean_comment(comment) \u001b[39mfor\u001b[39;00m comment \u001b[39min\u001b[39;00m row]\n\u001b[1;32m---> 50\u001b[0m     writer\u001b[39m.\u001b[39;49mwriterow(cleaned_row)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\venv\\lib\\encodings\\cp1252.py:19\u001b[0m, in \u001b[0;36mIncrementalEncoder.encode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39;49mcharmap_encode(\u001b[39minput\u001b[39;49m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,encoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\u043e' in position 0: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_comment(comment):\n",
    "    # Convertir en minuscules\n",
    "    comment = comment.lower()\n",
    "    \n",
    "    # Supprimer les caractères spéciaux et les chiffres\n",
    "    comment = ''.join(e for e in comment if e.isalnum() or e.isspace())\n",
    "    \n",
    "    return cleaned_comment\n",
    "\n",
    "# Ouvrir le fichier CSV\n",
    "with open('commentaires.csv', 'r', encoding='utf-8') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)  # Ignore la première ligne\n",
    "    \n",
    "    # Créer un nouveau fichier CSV pour stocker les commentaires nettoyés\n",
    "    with open('cleaned_comments.csv', 'w', newline='') as new_file:\n",
    "        writer = csv.writer(new_file)\n",
    "        writer.writerow(header)  # Copier la première ligne du fichier d'origine\n",
    "        \n",
    "        # Nettoyer les commentaires et les écrire dans le nouveau fichier CSV\n",
    "        for row in reader:\n",
    "            cleaned_row = [clean_comment(comment) for comment in row]\n",
    "            writer.writerow(cleaned_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Lire le fichier CSV avec les commentaires\n",
    "comments_df = pd.read_csv('comments.csv')\n",
    "\n",
    "# Analyser le sentiment de chaque commentaire\n",
    "for index, row in comments_df.iterrows():\n",
    "    # Créer un objet TextBlob pour le commentaire\n",
    "    blob = TextBlob(row['comment'])\n",
    "    # Calculer la polarité du sentiment\n",
    "    sentiment_polarity = blob.sentiment.polarity\n",
    "    # Classer le commentaire en fonction de sa polarité\n",
    "    if sentiment_polarity > 0:\n",
    "        sentiment = 'Positive'\n",
    "    elif sentiment_polarity < 0:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    # Ajouter la colonne \"sentiment\" au DataFrame\n",
    "    comments_df.at[index, 'sentiment'] = sentiment\n",
    "\n",
    "# Afficher les commentaires avec leur sentiment\n",
    "print(comments_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from textblob import TextBlob\n",
    "import csv\n",
    "\n",
    "# Connect to Neo4j database\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "session = driver.session()\n",
    "\n",
    "# Open the CSV file containing the comments\n",
    "with open('comments.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    comments = list(reader)\n",
    "\n",
    "# Analyze the sentiment of each comment and store it in Neo4j\n",
    "for comment in comments:\n",
    "    # Create a TextBlob object for the comment\n",
    "    blob = TextBlob(comment[0])\n",
    "    # Calculate the polarity of the sentiment\n",
    "    polarity = blob.sentiment.polarity\n",
    "    # Classify the comment based on its polarity\n",
    "    if polarity > 0:\n",
    "        sentiment = \"Positive\"\n",
    "    elif polarity < 0:\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        sentiment = \"Neutral\"\n",
    "    # Create a node for the comment and store its sentiment score\n",
    "    result = session.run(\"CREATE (c:Comment {text: $text, sentiment: $sentiment})\", text=comment[0], sentiment=sentiment)\n",
    "\n",
    "# Close the Neo4j session\n",
    "session.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
